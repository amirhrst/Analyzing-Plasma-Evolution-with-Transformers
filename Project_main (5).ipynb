{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Group 2315: Analyzing Plasma Evolution with Transformers\n",
        "\n",
        "#### Members:\n",
        "- Amirhossein Rostami\n",
        "- Davide Checchia\n",
        "- Jelin Raphael Akkara - 2072064\n",
        "- Kiamehr Javid"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SiBxGJA7oXHV"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.utils.class_weight as class_weight\n",
        "#from tensorflow import keras\n",
        "#from tensorflow.keras import backend as K\n",
        "#from keras import layers\n",
        "#from keras.callbacks import LearningRateScheduler\n",
        "from matplotlib.lines import Line2D\n",
        "from sklearn.metrics import roc_curve, confusion_matrix\n",
        "from sklearn.model_selection import KFold, train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn.decomposition import PCA\n",
        "#from keras.layers import *\n",
        "#from keras.models import *\n",
        "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "#import os\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')\n",
        "\n",
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "for device in gpu_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)\n",
        "\n",
        "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/lib/cuda/'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nNAeNXiStZof"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "pvTNMHfeoXHa"
      },
      "outputs": [],
      "source": [
        "#Returns x with shape(Total Valid Windows, Window length, Features), y with shape(Total Valid Windows,)\n",
        "def load_timeseries(path, cols_select, train_window=150, pred_time_gap=10, pred_window=0, acceptance_threshold=0.2, \n",
        "                    plot_param=False, plot_param_pos=2, \n",
        "                    pca=True, pca_num=15, plot_pca_var=False):  \n",
        "                    \n",
        "    data = np.genfromtxt(path, delimiter=',', skip_header=1)\n",
        "    colnames = pd.read_csv(path, index_col=0, nrows=0).columns.tolist()\n",
        "    x, y = [], []\n",
        "\n",
        "    for i in range(len(data)-train_window-pred_time_gap):\n",
        "      label = float(1.0 in data[i+train_window+pred_time_gap-pred_window : i+train_window+pred_time_gap+1, 3])        #Checks if 1 occurs in pred_window cells after pred_time_gap steps\n",
        "\n",
        "      if label == 0 and np.random.uniform(0, 1) > acceptance_threshold: continue  #Accepts a certain percentage of zero labels randomly\n",
        "\n",
        "      if 1.0 not in data[i:i+train_window+pred_time_gap-pred_window-1, 3]:                                          #If window does not have crash as 1 in it, append\n",
        "          if cols_select == ':': \n",
        "            col_unwanted = [0, 1]                                                       #Unwanted feature column indices\n",
        "            col_indices = [i for i in range(0, data.shape[1]) if i not in col_unwanted] #Selects all column indices except unwanted ones\n",
        "            x.append(data[i:i+train_window, col_indices])                               #Selects all features\n",
        "          else: x.append(data[i:i+train_window, cols_select])                           #Only takes specific (cols_select) features          \n",
        "          y.append(label)\n",
        "          if len(y) == 1: prim_arg = i\n",
        "\n",
        "    if len(np.shape(x)) == 2: x = np.expand_dims(x, axis=2)       #If only one feature, expand dimension\n",
        "    x, y = np.array(x), np.array(y)\n",
        "\n",
        "\n",
        "    # Applying PCA \n",
        "    if pca: \n",
        "\n",
        "      #Shape of x before PCA\n",
        "      print(\"Shape of the x before PCA\",x.shape)    \n",
        "\n",
        "      if pca_num > len(cols_select) and cols_select != ':': \n",
        "        print('Invalid Entry: The pca_num is greater than number of selected features.')\n",
        "        return()\n",
        "\n",
        "      reshaped_x = np.reshape(x, (x.shape[0]*x.shape[1], x.shape[2]))\n",
        "\n",
        "      scaler = StandardScaler()                       #Standardizes the features\n",
        "      data_scaled = scaler.fit_transform(reshaped_x)\n",
        "      pca = PCA(n_components=pca_num)                 # Specify the number of components you want to keep\n",
        "\n",
        "      x_pca = pca.fit_transform(data_scaled)\n",
        "      var_ratios = pca.explained_variance_ratio_      #Gets variance contribution of each pca component\n",
        "      #print('Variance Contribution of each pca component: ', var_ratios)\n",
        "      print('Total Variance covered: ', sum(var_ratios))\n",
        "\n",
        "      if plot_pca_var:  #Plots progression of variance covered as number of chosen components change\n",
        "        plt.plot(np.cumsum(var_ratios))\n",
        "        plt.xlabel('Number of Components Chosen')\n",
        "        plt.ylabel('Variance Covered')\n",
        "        plt.grid(True)\n",
        "        plt.title('Variance covered wrt Components Chosen')\n",
        "\n",
        "      x = np.reshape(x_pca, (x.shape[0],x.shape[1], pca_num))\n",
        "      print(\"PCA Successful, Shape of the pca data is : \",x.shape)\n",
        "\n",
        "    #Plotting\n",
        "    if plot_param:\n",
        "      time_vals = data[:, 1]\n",
        "      param_vals = data[:, plot_param_pos]\n",
        "      crash_times = data[data[:,3] == 1][:, 1]    #Times at which crashes occur\n",
        "\n",
        "      fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(25, 10), gridspec_kw={'height_ratios': [3, 1]})\n",
        "      fig.tight_layout(pad=7)\n",
        "\n",
        "      #First Plot - Plotting Parameter evolution\n",
        "      ax[0].plot(time_vals, param_vals)\n",
        "      for t in crash_times: ax[0].axvline(t, color='red', linestyle='--', lw=0.5, alpha=0.5)  #Plotting markers for crashes\n",
        "      ax[0].set_title('Time vs '+colnames[plot_param_pos-1], fontweight='bold')\n",
        "\n",
        "      #Second plot - Plotting training window, prediction window\n",
        "      ax[1].plot(time_vals, param_vals)\n",
        "      ax[1].set_xlim((min(time_vals)-500, time_vals[prim_arg+train_window+pred_time_gap]+500))  #Restrictin x range to windows\n",
        "      ax[1].set_title('Training and Prediction Windows', fontweight='bold')\n",
        "\n",
        "      #Plotting x window, prediction window in both plots\n",
        "      for pos in [0, 1]:            \n",
        "            window_t_low = time_vals[prim_arg]\n",
        "            window_t_up = time_vals[prim_arg + train_window]\n",
        "\n",
        "            ax[pos].axvline(window_t_low, color='black', lw=2, ls='--',  alpha=0.7, label='Train Window')           #Lower limit of window\n",
        "            ax[pos].axvline(window_t_up, color='black', lw=2, ls='--', alpha=0.7)                                   #Upper limit of window\n",
        "            ax[pos].axvspan(window_t_low, window_t_up, alpha=0.3, color='gray')                                     #Shades window region\n",
        "            \n",
        "            for i in range(pred_window+1):  #Upper limit of pred_time_gap\n",
        "              ax[pos].axvline(time_vals[prim_arg+train_window+pred_time_gap-i], color='darkgreen', lw=2, ls='--', alpha=1, label='Prediction')  \n",
        "\n",
        "            ax[pos].set_xlabel('Time (Alfven)')\n",
        "            ax[pos].set_ylabel(colnames[plot_param_pos-1])\n",
        "\n",
        "      #Plotting Legend\n",
        "      handles, labels = plt.gca().get_legend_handles_labels()   #Gets handles and labels\n",
        "\n",
        "      #Manually appending crash line to handles and labels\n",
        "      crash_line = Line2D([0], [0], color='red', linestyle='--')\n",
        "      handles.extend([crash_line])    \n",
        "      labels.extend(['Crash'])\n",
        "\n",
        "      by_label = dict(zip(labels, handles))   #Only takes in unique values\n",
        "      ax[0].legend(by_label.values(), by_label.keys(), loc='upper right')\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kSb40zUDoXHY"
      },
      "outputs": [],
      "source": [
        "Length = 31              #Window Length\n",
        "delta_t = 30             #Prediction length\n",
        "WR = 1                   #Prediction Window\n",
        "acceptance_perc = 0.2    #Percentage of zeros selected (Down-sampling)\n",
        "training_perc = 0.7      #Percentage used for training\n",
        "\n",
        "path = \"/content/drive/MyDrive/LCP_Project/merged_file.csv\"\n",
        "\n",
        "x, y = load_timeseries(path, ':', \n",
        "                       train_window=Length, \n",
        "                       pred_time_gap = delta_t,                    \n",
        "                       pred_window = WR,\n",
        "                       acceptance_threshold= acceptance_perc, \n",
        "                       plot_param=True, \n",
        "                       plot_param_pos=2, \n",
        "                       pca=False, \n",
        "                       pca_num=29,\n",
        "                       plot_pca_var=False)\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r6L72TILtjko"
      },
      "source": [
        "## Splitting to training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hogaVn6Ut8E",
        "outputId": "ad00c86a-d4eb-412e-af83-eb437df8866c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Classes:  2\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, train_size=training_perc)\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "print('Unique Classes: ', num_classes)\n",
        "\n",
        "rep = 0\n",
        "\n",
        "for i in range(rep):\n",
        "  x_train = np.concatenate((x_train,x_train[y_train==1]))\n",
        "  y_train = np.concatenate((y_train,y_train[y_train==1]))\n",
        "\n",
        "  idx = np.random.permutation(len(x_train))\n",
        "  x_train = x_train[idx]\n",
        "  y_train = y_train[idx]\n",
        "\n",
        "w = sum(y_train)/len(y_train)           #Gets proportion of ones in training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfr6pQpft7-0",
        "outputId": "761f0f7d-48cd-4ea9-bdd4-98b75abe2c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of 1 in training set:  1.604434072345391\n",
            "Percentage of 0 in training set:  98.39556592765462\n",
            "\n",
            "Percentage of 1 in testing set:  1.6164709885996258\n",
            "Percentage of 0 in testing set:  98.38352901140037\n"
          ]
        }
      ],
      "source": [
        "zeros_train, ones_train = np.bincount(y_train.astype(int))   #Gets counts of 0, 1 in training set\n",
        "zeros_test, ones_test = np.bincount(y_test.astype(int))   #Gets counts of 0, 1 in testing set\n",
        "\n",
        "ones_train_perc = 100*(ones_train/len(y_train))   #Percentage division in training set\n",
        "zeros_train_perc = 100 - ones_train_perc          \n",
        "\n",
        "ones_test_perc = 100*(ones_test/len(y_test))    #Percentage division in testing set\n",
        "zeros_test_perc = 100 - ones_test_perc\n",
        "\n",
        "print('Percentage of 1 in training set: ', ones_train_perc)\n",
        "print('Percentage of 0 in training set: ', zeros_train_perc)\n",
        "\n",
        "print('\\nPercentage of 1 in testing set: ', ones_test_perc)\n",
        "print('Percentage of 0 in testing set: ', zeros_test_perc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_list = [\n",
        "      #keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
        "      #keras.metrics.MeanSquaredError(name='Brier score'),\n",
        "      # keras.metrics.TruePositives(name='tp'),\n",
        "      #keras.metrics.FalsePositives(name='fp'),\n",
        "      #keras.metrics.TrueNegatives(name='tn'),\n",
        "      #keras.metrics.FalseNegatives(name='fn'), \n",
        "      'accuracy',\n",
        "    #   keras.metrics.BinaryAccuracy(name='bin_accuracy'),\n",
        "       keras.metrics.Precision(name='precision'),\n",
        "       keras.metrics.Recall(name='recall'),\n",
        "    #   keras.metrics.AUC(name='auc'),\n",
        "      #keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-------------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jNucJwI_w2KF"
      },
      "source": [
        "## Define Models and Compile"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgzs3tXf7WV5"
      },
      "outputs": [],
      "source": [
        "# TRANSFORMER\n",
        "shape = x_train.shape[1:]\n",
        "\n",
        "\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0, kernel_size=1 , act = 'gelu'):\n",
        "    # Normalization and Attention\n",
        "    x = inputs\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=kernel_size, activation=act, padding=\"same\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    return x + res\n",
        "\n",
        "\n",
        "def build_model(\n",
        "    input_shape=shape,\n",
        "    head_size=256,\n",
        "    num_heads=1,\n",
        "    ff_dim=16,\n",
        "    kernel_size=1,\n",
        "    num_transformer_blocks=3,\n",
        "    mlp_units=[],\n",
        "    act = 'gelu',\n",
        "    dropout=0.5,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    K.clear_session()\n",
        "    inputs = keras.Input(shape=(input_shape))\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout, kernel_size , act)\n",
        "\n",
        "    x = layers.Flatten(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=act)(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs) \n",
        "\n",
        "    model.compile(loss = 'binary_crossentropy' , metrics = metrics_list , optimizer = keras.optimizers.Adam(learning_rate=1e-3))\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model(shape,\n",
        "    head_size = 256,\n",
        "    num_heads = 1,\n",
        "    ff_dim = 16,\n",
        "    kernel_size=1,\n",
        "    num_transformer_blocks = 3,\n",
        "    mlp_units=[],\n",
        "    mlp_dropout=0.5,\n",
        "    dropout=0.5)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6QdElCTwoXHc"
      },
      "outputs": [],
      "source": [
        "def make_cnn_model(input_shape = x_train.shape[1:] , num_layers = 3 , kernel = 5 , filters = 16 , do_batch_norm = True , act = 'relu',dropout= 0 ):    \n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    l = input_layer\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "\n",
        "      l = keras.layers.Conv1D(filters=filters, kernel_size=kernel, padding=\"same\")(l)\n",
        "      l = keras.layers.Dropout(dropout)(l)\n",
        "      if do_batch_norm : l = keras.layers.BatchNormalization()(l)\n",
        "      l = keras.layers.Activation(act)(l)   #activation for layers \n",
        "      \n",
        "    gap = keras.layers.Flatten()(l)   #Flattens\n",
        "    #gap = keras.layers.GlobalAveragePooling1D()(conv1)\n",
        "\n",
        "    output_layer = keras.layers.Dense(1, activation='sigmoid')(gap) #activation fro output layer \n",
        "\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "    \n",
        "    model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=8*1e-3),\n",
        "    loss=[\"binary_crossentropy\"],\n",
        "    \n",
        "    metrics=[keras.metrics.TruePositives(name='tp'), \n",
        "             keras.metrics.BinaryAccuracy(name='bin_accuracy'), \n",
        "             keras.metrics.Precision(name='precision'),\n",
        "             keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = make_cnn_model(x_train.shape[1:])\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_dnn(input_shape=x_train.shape[1:] , dims = [2038,1024,512,256,128,64,32,16,8] , act = 'elu' , dropout = 0):\n",
        "  \n",
        "  \n",
        "  inputs = input_layer = keras.layers.Input(input_shape)\n",
        "  l = inputs\n",
        "\n",
        "  l = keras.layers.Flatten()(l)  \n",
        "  for dim in dims:\n",
        "    l = keras.layers.Dense(dim , activation = act )(l)\n",
        "    l = keras.layers.Dropout(dropout)(l)\n",
        "  output = keras.layers.Dense(1,activation = 'sigmoid')(l)\n",
        "\n",
        "  model = keras.models.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=[\"binary_crossentropy\"],\n",
        "    metrics=[keras.metrics.TruePositives(name='tp'), \n",
        "             keras.metrics.BinaryAccuracy(name='bin_accuracy'), \n",
        "             keras.metrics.Precision(name='precision'),\n",
        "             keras.metrics.Recall(name='recall')]\n",
        "             )\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "model = make_dnn(x_train.shape[1:])\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validating Model (Stratified K-Fold Cross Validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {'filters': [8, 16, 32, 64], \n",
        "              'kernel': [1, 3, 4, 6], \n",
        "              'num_layers': [1, 3, 6]}\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch % 3 == 0 and epoch > 0:\n",
        "        lr = lr / 2\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#Using class_weights assigned by keras\n",
        "#class_weights = class_weight.compute_class_weight(class_weight = 'balanced', classes=np.unique(y_train), y = y_train) #Assigns weights to each class wrt counts\n",
        "#class_weights = {i : class_weights[i] for i in range(len(class_weights))}   #Class weight dictionary\n",
        "\n",
        "model_search = KerasClassifier(build_fn=make_cnn_model, verbose=2 , epochs = 30 , callbacks = [lr_scheduler] , class_weight = {0:w*10,1:(1-w)*10})\n",
        "grid_search = GridSearchCV(estimator=model_search, param_grid=param_grid, cv=3 ,scoring=[\"accuracy\", \"recall\", \"roc_auc\"], refit=\"accuracy\" , verbose = 3)\n",
        "\n",
        "\n",
        "res = grid_search.fit(x_train, y_train , verbose = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save Grid Search Results\n",
        "\n",
        "results = pd.DataFrame(res.cv_results_)\n",
        "# results.sort_values(by='accuracy', inplace=True)\n",
        "results.to_excel(\"DNN_GS_res.xlsx\")\n",
        "results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i47cFoCh1nju"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch % 6 == 0 and epoch > 0:\n",
        "        lr = lr / 2\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#class_weights = class_weight.compute_class_weight(class_weight = 'balanced', classes=np.unique(y_train), y = y_train) #Assigns weights to each class wrt counts\n",
        "#class_weights = {i : class_weights[i] for i in range(len(class_weights))}   #Class weight dictionary\n",
        "\n",
        "history = model.fit(x_train , y_train , \n",
        "                        validation_split = 0.1 , \n",
        "                        epochs = 60 , \n",
        "                        callbacks = lr_scheduler , \n",
        "                        batch_size = 200 , \n",
        "                        class_weight = {0:w,1:1-w})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "04DqvgKy2475"
      },
      "source": [
        "## Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRBx3kAKzCip",
        "outputId": "829ae412-efbb-44a4-80d2-ee1e7dfdb3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "92/92 [==============================] - 0s 3ms/step\n",
            "40/40 [==============================] - 0s 3ms/step\n",
            "(13712, 1) (5877, 1)\n"
          ]
        }
      ],
      "source": [
        "#Predictions from model with training and testing sets\n",
        "batch_size=200\n",
        "pred_train = model.predict(x_train, batch_size=batch_size)\n",
        "pred_test = model.predict(x_test, batch_size=batch_size)\n",
        "print(np.shape(pred_train), np.shape(pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URN2ZUD6ndby",
        "outputId": "8a6282da-271a-4ba4-d60b-e98319eb7a4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.04561721161007881\n",
            "tp: 66.0\n",
            "accuracy: 0.0\n",
            "bin_accuracy: 0.9902480840682983\n",
            "precision: 0.7021276354789734\n",
            "recall: 0.6947368383407593\n",
            "auc: 0.9163697957992554\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "\n",
        "for metric, value in zip(model.metrics_names, results): #Prints metric scores\n",
        "  print(f'{metric}: {value}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-------------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Processing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_matrix_df(df, param_colnames, score_colname):       #Converts df to matrix, unique param values for heatmap\n",
        "\n",
        "    fil_cols = param_colnames + [score_colname]             #Gets list of column names to be filtered from DataFrame\n",
        "    df_fil = df[fil_cols].sort_values(param_colnames)       #Gets filtered Dataframe, sorts according to ordered param list\n",
        "\n",
        "    distinct_param_vals = [np.unique(df_fil[col]) for col in param_colnames]    #Gets distinct param values for each param\n",
        "\n",
        "    mat_dim = tuple([len(pvals) for pvals in distinct_param_vals])              #Matrix dimension wrt number of unique param values\n",
        "    matrix = np.array(df_fil[score_colname]).reshape(mat_dim)                   #Gets ordered matrix\n",
        "\n",
        "    return matrix, distinct_param_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Converts text file to data frame\n",
        "def text_to_df(path, param_indices, add_fbeta=True, beta=3):\n",
        "\n",
        "    score_dict = {'L': [], 'dt': [], 'accept_perc': [], 'WR': [], 'pca': [],\n",
        "                'loss': [], 'tp': [], 'accuracy': [], 'recall': [], 'precision': []}\n",
        "\n",
        "    with open(path, 'r') as file:\n",
        "        vals = file.readlines()[6:]         #Gets each line in the file into a list\n",
        "\n",
        "    for lcount, line in enumerate(vals):    #Iterates through each line\n",
        "        if line == '\\n': continue           #Ignores new line statement\n",
        "        line = re.sub(',', '', line)        #Removes ','\n",
        "        line_split = line.split()           #Splits line string into list of elements\n",
        "\n",
        "        if line_split[0] == 'MODEL':         \n",
        "\n",
        "            for i in range(len(param_indices)):     #Gets param values (L, dt, accept_perc, WR, pca)\n",
        "                key = list(score_dict.keys())[i]    #Corresponding key in score_dict wrt iteration\n",
        "\n",
        "                if i == 2: score_dict[key].append( float( line_split[param_indices[i]][:-1] ) / 100)    #Gets accept_perc as float\n",
        "                elif i == 4: score_dict[key].append( ( line_split[param_indices[i]] ))                  #Gets pca as string\n",
        "                else : score_dict[key].append( int( line_split[param_indices[i]] ))                     #Gets rest of metrics as integer\n",
        "\n",
        "            for i in range(0, 5):                   #Gets metric values (loss, tp, accuracy, recall, precision)\n",
        "                key = list(score_dict.keys())[i+5]  #Corresponding key in score_dict wrt iteration\n",
        "                score_dict[key].append(float( vals[lcount+i+1].split()[1] ))    #Gets value as float\n",
        "    \n",
        "    df = pd.DataFrame(score_dict)   #Converts to Dataframe\n",
        "    if add_fbeta:                   #Adds f_beta score column\n",
        "        df['fbeta_score'] = (1+beta**2) / ((beta**2)*(1/df['recall']) + (1/df['precision']))\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Obtaining dataframe from txt file\n",
        "files = ['L_dt_PCAFalse_WR0.txt', 'L_dt_PCATrue_WR0.txt', 'L_dt_PCAFalse_WR1.txt', 'L_dt_PCATrue_WR1.txt']\n",
        "path = 'Results/Transformers/L_dt_PCA_WR/'\n",
        "param_indices = [4, 7, 10, 13, 16]      #Positions of metrics in string\n",
        "beta_val=5                              #Initializes beta value for F_score \n",
        "\n",
        "df_w0_pca0 = text_to_df(path+files[0], param_indices, add_fbeta=True, beta=beta_val).groupby(by=['L', 'dt']).mean().reset_index()\n",
        "df_w0_pca1 = text_to_df(path+files[1], param_indices, add_fbeta=True, beta=beta_val).groupby(by=['L', 'dt']).mean().reset_index()\n",
        "df_w1_pca0 = text_to_df(path+files[2], param_indices, add_fbeta=True, beta=beta_val).groupby(by=['L', 'dt']).mean().reset_index()\n",
        "df_w1_pca1 = text_to_df(path+files[3], param_indices, add_fbeta=True, beta=beta_val).groupby(by=['L', 'dt']).mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For Heatmap: Getting matrix, unique param values for each case\n",
        "matrix_00, pvals_00 = get_matrix_df(df_w0_pca0, ['L', 'dt'], 'fbeta_score')\n",
        "matrix_01, pvals_01 = get_matrix_df(df_w0_pca1, ['L', 'dt'], 'fbeta_score')\n",
        "matrix_10, pvals_10 = get_matrix_df(df_w1_pca0, ['L', 'dt'], 'fbeta_score')\n",
        "matrix_11, pvals_11 = get_matrix_df(df_w1_pca1, ['L', 'dt'], 'fbeta_score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For Line Plot:  Adding column 'total window' and grouping wrt it\n",
        "df_w0_pca0['total_window'] = df_w0_pca0['L'] + df_w0_pca0['dt']\n",
        "df_w0_pca1['total_window'] = df_w0_pca1['L'] + df_w0_pca1['dt']\n",
        "df_w1_pca0['total_window'] = df_w1_pca0['L'] + df_w1_pca0['dt']\n",
        "df_w1_pca1['total_window'] = df_w1_pca1['L'] + df_w1_pca1['dt']\n",
        "\n",
        "df_w0_pca0 = df_w0_pca0.groupby(by=['total_window']).mean().reset_index()\n",
        "df_w0_pca1 = df_w0_pca1.groupby(by=['total_window']).mean().reset_index()\n",
        "df_w1_pca0 = df_w1_pca0.groupby(by=['total_window']).mean().reset_index()\n",
        "df_w1_pca1 = df_w1_pca1.groupby(by=['total_window']).mean().reset_index()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cjUXJDKB81VU"
      },
      "source": [
        "## Plotting"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting: Metric from model history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "RAH6fr7V-eue"
      },
      "outputs": [],
      "source": [
        "def plot_metric(metric, y_vals, y_type='Train', div_y=False, add_plot=False):  #Plots metric progression against all y values\n",
        "\n",
        "  if div_y:   #Divides metric values by total y_vals\n",
        "    plt.plot(history.history[metric]/sum(y_vals), color='#1f77b4')          #Plots training curve\n",
        "    plt.plot(history.history['val_'+metric]/sum(y_vals), color='orange')    #Plots validation curve\n",
        "  else: \n",
        "    plt.plot(history.history[metric], color='#1f77b4')\n",
        "    plt.plot(history.history['val_'+metric], color='orange')\n",
        "\n",
        "  plt.grid(True)\n",
        "  plt.title(f'Model {metric}')\n",
        "  plt.ylabel(metric, fontsize=\"large\")\n",
        "  plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "  plt.legend([y_type, \"Validation\"], loc=\"lower right\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "eKc43pC792aD"
      },
      "outputs": [],
      "source": [
        "def plot_roc(name, labels, predictions, **kwargs):    #Plots ROC Curve\n",
        "  fp, tp, _ = roc_curve(labels, predictions)          #Gets false positives, true positives\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, color='blue', **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  #plt.xlim([-0.5,20])\n",
        "  #plt.ylim([80,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "yUerVrIZP5GM",
        "outputId": "25529182-f73e-45e0-89ef-97cae2e27344"
      },
      "outputs": [],
      "source": [
        "#Plotting metric\n",
        "plot_metric('recall', y_train, 'Train', div_y=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "7iML2a7VAxQ8",
        "outputId": "50b5ad23-d9e0-4c56-af9e-eefec1ecc45c"
      },
      "outputs": [],
      "source": [
        "#Plotting ROC Curve\n",
        "plot_roc('Train Curve', y_train, pred_train)\n",
        "plot_roc('Test Curve', y_test, pred_test, linestyle='--')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting: Confusion Matrix for model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "Bw3RnX4lflsY"
      },
      "outputs": [],
      "source": [
        "def plot_cm(ax, labels, predictions, data_type, threshold=0.5, color_simple=False, label_order = [1, 0]):    #Plots confusion matrix\n",
        "\n",
        "    cm = confusion_matrix(labels, predictions > threshold, labels=label_order)    #Gets confusion matrix counts as array\n",
        "\n",
        "    row_sums = np.sum(cm, axis = 1)                           #Gets sum along matrix rows\n",
        "    row_sums_mat = np.tile(row_sums, (cm.shape[0], 1)).T      #Converts row sum values into shape of confusion matrix\n",
        "    cm_percs = cm / row_sums_mat                              #Gets confusion matrix with rows divided by row sums\n",
        "\n",
        "    cm_percs_str = [\"{0:.2%}\".format(perc) for perc in cm_percs.flatten()]                          #Flattens cm, expresses percents in 2 decimal places\n",
        "    annot_labels = [f'  {perc} \\n ( {count} )' for perc, count in zip(cm_percs_str, cm.flatten())]  #Prepares annotation labels\n",
        "    annot_labels = np.array(annot_labels).reshape((2, 2))                                           #Gets annot labels in shape of confusion matrix\n",
        "  \n",
        "    if color_simple: cmap = sns.color_palette('Blues', as_cmap=True)  #One dimensional Color Map\n",
        "    else:                                                             #Diverging Color Map\n",
        "      cmap = sns.color_palette(\"vlag\", as_cmap=True)                            \n",
        "      cmap = list(reversed(cmap.colors))\n",
        "\n",
        "    #Plots confusion matrix using heatmap, label order is the ordering of axis labels ((1, 0) or (0, 1))\n",
        "    hm = sns.heatmap(cm_percs*100, annot=annot_labels, fmt=\"\", cmap=cmap, ax=ax, yticklabels=label_order, xticklabels=label_order)                    \n",
        "\n",
        "    hm.set_title(f'{data_type} : Confusion matrix with threshold = {threshold}', fontsize=11, pad=10)\n",
        "    hm.set_ylabel('Actual label')\n",
        "    hm.set_xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "z9ON5zFvwsZi",
        "outputId": "a7cf71cf-7788-4b66-e03f-2435fd23d789"
      },
      "outputs": [],
      "source": [
        "#Plotting only Training Set\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 7))\n",
        "plot_cm(ax, y_train, pred_train, data_type='Train Data', threshold=0.5, color_simple=False, label_order = [1, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "You274-Z3URm",
        "outputId": "d9630f0d-d230-4d6e-e7be-5ec38bb45953"
      },
      "outputs": [],
      "source": [
        "#Plotting both training and testing set\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n",
        "fig.tight_layout(pad=7)\n",
        "\n",
        "plot_cm(ax[0], y_train, pred_train, data_type='Train Data', threshold=0.5, color_simple=False, label_order = [1, 0])\n",
        "plot_cm(ax[1], y_test, pred_test, data_type='Test Data', threshold=0.5, color_simple=True, label_order = [1, 0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting: Threshold Analysis -\n",
        "- Plotting Histogram of prediction values\n",
        "- Plotting progression of precision, recall, F_score\n",
        "- Plotting progression of True Positives, True Negatives, False Positives, False Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "3Y85MRePy8NQ"
      },
      "outputs": [],
      "source": [
        "def get_opt_threshold(pred_vals, beta_val=5, plot_precall=False, plot_each=False, plot_hist=False, n_bins=10, yrange=(0, 500)): #Gets optimal threshold\n",
        "  threshold_vals = np.linspace(0, 1, 100)[1:-1]         #Array of threshold values to plot over\n",
        "  precision_vals, recall_vals = [], []              \n",
        "  tp_vals, tn_vals, fp_vals, fn_vals = [], [], [], []\n",
        "\n",
        "  for thr_val in threshold_vals:    #Gets metric values for each threshold\n",
        "    cm = confusion_matrix(y_train, pred_vals > thr_val, labels=[1, 0])\n",
        "    tp, tn, fp, fn = cm[0, 0], cm[1, 1], cm[1, 0], cm[0, 1]\n",
        "\n",
        "    if plot_each: \n",
        "      tp_vals.append(tp)\n",
        "      tn_vals.append(tn)\n",
        "      fp_vals.append(fp)\n",
        "      fn_vals.append(fn)\n",
        "\n",
        "    precision_vals.append( tp / (tp + fp) )\n",
        "    recall_vals.append( tp / (tp + fn) )\n",
        "\n",
        "  #Inverses precision, recall values\n",
        "  precision_inv = 1/np.array(precision_vals)\n",
        "  recall_inv = 1/np.array(recall_vals)\n",
        "  \n",
        "  fbeta_vals = (1 + beta_val**2) / ((beta_val**2 * recall_inv) + precision_inv)   #Gets F_beta score wrt given beta value\n",
        "  f_arg_max = np.argwhere(fbeta_vals == max(fbeta_vals))                          #Gets argument for maximum F_beta score\n",
        "  opt_threshold = threshold_vals[f_arg_max][0, 0]                                 #Gets maximum threshold\n",
        "\n",
        "  print('Threshold with max f value: ', opt_threshold)\n",
        "\n",
        "  if plot_hist:   #Plots histogram of prediction values\n",
        "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8, 6), gridspec_kw={'height_ratios': [3, 1]})\n",
        "    fig.tight_layout(pad=1)\n",
        "\n",
        "    pred_flat = pred_vals.flatten()\n",
        "    ax[0].hist(pred_flat, bins=n_bins, alpha=0.9, range=(0, 1))   #Plots histogram of predicted values\n",
        "    ax[0].set_title('Histogram: Training Predictions')\n",
        "    ax[0].grid(True, alpha=0.5)\n",
        "    ax[0].set_ylabel('Counts')\n",
        "\n",
        "    ax[1].hist(pred_flat, bins=n_bins, alpha=0.9, range=(0, 1))   #Plots close-up version for low count values\n",
        "    ax[1].grid(True, alpha=0.5)\n",
        "    ax[1].set_ylim(yrange)\n",
        "    ax[1].set_xlabel('Prediction')\n",
        "    ax[1].set_ylabel('Counts')\n",
        "\n",
        "\n",
        "  if plot_precall:    #Plots progression of precision, recall, F_beta score wrt threshold\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "    fig.tight_layout(pad=5)\n",
        "\n",
        "    ax[0].plot(threshold_vals, recall_vals, label='Recall')         #Plots recall curve\n",
        "    ax[0].plot(threshold_vals, precision_vals, label='Precision')   #Plots precision curve\n",
        "    ax[0].set_ylabel('Value')\n",
        "    ax[0].set_title('Variation across Thresholds')\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(threshold_vals, fbeta_vals)\n",
        "    ax[1].set_ylabel('F Value')\n",
        "    ax[1].set_title(f'F-{beta_val} Metric across Thresholds')\n",
        "    ax[1].text(opt_threshold-0.05, 0.5, 'Optimal Threshold: {:.2f}'.format(opt_threshold), rotation=90)\n",
        "\n",
        "    for i in [0, 1]:\n",
        "        ax[i].grid(True, alpha=0.7)\n",
        "        ax[i].set_xlabel('Threshold')\n",
        "        ax[i].axvline(opt_threshold, color='red', linestyle='--', linewidth=0.8)\n",
        "\n",
        "  if plot_each:     #Plots progression of (tp, fn, tn, fp) wrt threshold\n",
        "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 7))\n",
        "    fig.tight_layout(pad=5)\n",
        "\n",
        "    ax[0, 0].plot(threshold_vals, tp_vals)\n",
        "    ax[0, 0].set_title('True Positives')\n",
        "\n",
        "    ax[0, 1].plot(threshold_vals, fn_vals, color='firebrick')\n",
        "    ax[0, 1].set_title('False Negatives')\n",
        "\n",
        "    ax[1, 0].plot(threshold_vals, fp_vals, color='firebrick')\n",
        "    ax[1, 0].set_title('False Positives')\n",
        "\n",
        "    ax[1, 1].plot(threshold_vals, tn_vals)\n",
        "    ax[1, 1].set_title('True Negatives')\n",
        "\n",
        "    for i in [0, 1]:\n",
        "      for j in [0, 1]:\n",
        "        ax[i, j].grid(True, alpha=0.7)\n",
        "        ax[i, j].set_xlabel('Threshold')\n",
        "        ax[i, j].set_ylabel('Counts')\n",
        "\n",
        "    return (opt_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "WpYijWN31iYA",
        "outputId": "b536039d-8193-4caa-8107-f72bbe5ab002"
      },
      "outputs": [],
      "source": [
        "opt_threshold = get_opt_threshold(pred_train, beta_val=15, plot_precall=False, plot_each=True, plot_hist=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting: Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_heatmap(ax, matrix_vals, param0_vals, param1_vals,     #Plots 2D heatmap given ordered matrix, unique param values\n",
        "                    title, xlabel, ylabel, cbar_label,\n",
        "                    normalize_mat=False, \n",
        "                    flip_vert=False, \n",
        "                    color=0, invert_cmap=False,\n",
        "                    annotate=False,\n",
        "                    highlight_max_cells=3):     \n",
        "\n",
        "    if normalize_mat:       #Normalizes matrix with max, min values\n",
        "        max_val = np.max(matrix_vals)\n",
        "        min_val = np.min(matrix_vals)\n",
        "\n",
        "        norm_matrix = (matrix_vals - min_val) / (max_val - min_val)\n",
        "        matrix_vals = norm_matrix\n",
        "\n",
        "        title = title + \" \\n ( Max: {0:.2f}, Min: {1:.2f} )\".format(max_val, min_val)\n",
        "\n",
        "    if flip_vert:           #Flips matrix vertically\n",
        "        matrix_vals = np.flipud(matrix_vals)\n",
        "        param0_vals = param0_vals[::-1]\n",
        "\n",
        "    #Choosing color\n",
        "    if color == 0:   cmap = sns.color_palette(\"vlag\", as_cmap=True)    \n",
        "    elif color == 1: cmap = sns.diverging_palette(220, 20, as_cmap=True)      \n",
        "    elif color == 2: cmap = sns.color_palette(\"flare\", as_cmap=True)\n",
        "    else: cmap = sns.diverging_palette(250, 30, l=65, center=\"dark\", as_cmap=True)\n",
        "    \n",
        "    if invert_cmap: cmap = cmap.reversed()  #Inverts color map\n",
        "\n",
        "    #Plots heatmap\n",
        "    hm = sns.heatmap(matrix_vals, annot=annotate, cmap=cmap, ax=ax,\n",
        "                        xticklabels=param1_vals, yticklabels=param0_vals,\n",
        "                        vmin=0, vmax=1)\n",
        "\n",
        "    hm.set_title(title, fontweight='bold')\n",
        "    hm.set_xlabel(xlabel)\n",
        "    hm.set_ylabel(ylabel)\n",
        "\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.set_label(cbar_label, labelpad=10)\n",
        "\n",
        "    if highlight_max_cells !=0:     #Highlights Cells with n maximum values\n",
        "        rounded_matrix = np.round(matrix_vals.flatten(), 5)                     #Rounds matrix for comparison, avoiding floating point errors\n",
        "        max_vals = np.sort(np.unique(rounded_matrix))[-highlight_max_cells:]    #Gets n maximum values\n",
        "        max_args = [np.where(rounded_matrix == val)[0] for val in max_vals]     #Gets corresponding arguments for the maximum values\n",
        "\n",
        "        for args in max_args:\n",
        "            for ind in args:\n",
        "                row, col = np.unravel_index(ind, matrix_vals.shape)\n",
        "\n",
        "                hm.add_patch(plt.Rectangle((col, row), 1, 1, fill=False,    #Highlights border of chosen cell\n",
        "                                edgecolor='black', lw=2, linestyle='--'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotting L-dt Heatmap (F Score as measure)\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
        "fig.tight_layout(pad=8)\n",
        "fig.suptitle(f'Plotting L vs dt \\n (Normalized F_{beta_val} Score as measure)', fontweight='bold', fontsize=15)\n",
        "plt.subplots_adjust(top=0.89)\n",
        "\n",
        "axes_list = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "matrix_list = [matrix_00, matrix_01, matrix_10, matrix_11]\n",
        "pvals_list = [pvals_00, pvals_01, pvals_10, pvals_11]\n",
        "\n",
        "for i in range(len(axes_list)):\n",
        "    plot_heatmap(ax[axes_list[i][0], axes_list[i][1]], \n",
        "                    matrix_list[i], pvals_list[i][0], pvals_list[i][1],\n",
        "                    f'Heatmap (WR:{axes_list[i][0]}, PCA:{axes_list[i][1]}, Beta: {beta_val})', \n",
        "                    'dt', 'L', f'F{beta_val}_score', \n",
        "                    normalize_mat=True, flip_vert=True, \n",
        "                    color=1, invert_cmap=True,\n",
        "                    highlight_max_cells=3,\n",
        "                    annotate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotting L-dt Heatmap (Recall as measure)\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
        "fig.tight_layout(pad=6)\n",
        "fig.suptitle('Plotting L vs dt \\n (Recall as measure)', fontweight='bold', fontsize=15)\n",
        "plt.subplots_adjust(top=0.89)\n",
        "\n",
        "axes_list = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "matrix_list = [matrix_00, matrix_01, matrix_10, matrix_11]\n",
        "pvals_list = [pvals_00, pvals_01, pvals_10, pvals_11]\n",
        "\n",
        "for i in range(len(axes_list)):\n",
        "    plot_heatmap(ax[axes_list[i][0], axes_list[i][1]], \n",
        "                    matrix_list[i], pvals_list[i][0], pvals_list[i][1],\n",
        "                    f'Heatmap (WR:{axes_list[i][0]}, PCA:{axes_list[i][1]})', \n",
        "                    'dt', 'L', 'recall', \n",
        "                    normalize_mat=False, flip_vert=True, \n",
        "                    color=1, invert_cmap=True,\n",
        "                    highlight_max_cells=3,\n",
        "                    annotate=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting: F_Score reference plots with recall, precision for given beta value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_fbeta_ref(ax, beta=5, accuracy=0.1, annotate=False, color=0):  #Plots reference heatmap with recall vs precision for a given beta value\n",
        "\n",
        "    def f_beta(beta, precision, recall):        #Gets F_beta score\n",
        "        if recall == 0 or precision == 0: return 0\n",
        "        fbeta = (1+beta**2) / (((beta**2)/recall) + 1/precision)\n",
        "        return fbeta\n",
        "\n",
        "    precision_vals = np.round(np.arange(0, 1+accuracy, accuracy), 5)    #Array of precision values for heatmap\n",
        "    recall_vals = np.round(np.arange(0, 1+accuracy, accuracy), 5)       #Array of recall values for heatmap\n",
        "    fbeta_vals = [f_beta(beta, prec, recall) for prec in precision_vals for recall in recall_vals]  #Gets f_beta score for all combinations of precision, recall\n",
        "\n",
        "    matrix_fbeta = np.array(fbeta_vals).reshape((len(precision_vals), len(recall_vals)))    #Expresses fbeta_vals as ordered matrix\n",
        "\n",
        "    plot_heatmap(ax, matrix_fbeta, precision_vals, recall_vals, \n",
        "                title=f'Matrix Heatmap: (Beta: {beta})', xlabel='recall', ylabel='precision', cbar_label='F5_score',\n",
        "                normalize_mat=False,\n",
        "                flip_vert=True,\n",
        "                color=color,\n",
        "                invert_cmap=True,\n",
        "                annotate=annotate,\n",
        "                highlight_max_cells=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotting F_Beta Reference Heatmaps\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
        "fig.tight_layout(pad=6)\n",
        "\n",
        "axes_list = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "beta_vals = [0, 1, 2, 3]\n",
        "\n",
        "for i in range(len(axes_list)):\n",
        "    plot_fbeta_ref(ax[axes_list[i][0], axes_list[i][1]], beta_vals[i], \n",
        "                   annotate=True, color=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting: Line Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_df_line(ax, df, param0, param1, title, regress=False,  yrange=(0, 1)):     #Line plot given dataframe and parameter column names\n",
        "    #Plots line plot\n",
        "    ax.plot(df[param0], df[param1])\n",
        "\n",
        "    if regress: #Plots regression fit\n",
        "        sns.regplot(data=df, x=param0, y=param1, ax=ax, \n",
        "                        order=1,    #Order of regression polynomial\n",
        "                        ci=95,      #Confidence Interval\n",
        "                        marker='o',\n",
        "                        line_kws={'linewidth': 2, 'linestyle': '--', 'alpha': 0.7}\n",
        "                        )\n",
        "\n",
        "    ax.set_xlabel(param0)   \n",
        "    ax.set_ylabel(param1)\n",
        "    ax.set_ylim(yrange)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotting Line Plots: F_score vs dt\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
        "fig.tight_layout(pad=6)\n",
        "fig.suptitle(f'Plotting F_{beta_val} score vs dt', fontweight='bold', fontsize=15)\n",
        "\n",
        "axes_list = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "df_list = [df_w0_pca0, df_w0_pca1, df_w1_pca0, df_w1_pca1]\n",
        "\n",
        "for i in range(len(axes_list)):\n",
        "    plot_df_line(ax[axes_list[i][0], axes_list[i][1]],      #Mentioning Axis of figure\n",
        "                    df_list[i], 'dt', 'fbeta_score',             #Dataframe, params to be plotted\n",
        "                    regress=True,\n",
        "                    title=f'(W: {axes_list[i][0]}, pca: {axes_list[i][1]}, L: 45)', \n",
        "                    #yrange=(0.2, 0.8)\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotting Line Plots: Total Window vs Recall\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
        "fig.tight_layout(pad=6)\n",
        "fig.suptitle(f'Plotting Total Window vs Recall', fontweight='bold', fontsize=15)\n",
        "\n",
        "axes_list = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "df_list = [df_w0_pca0, df_w0_pca1, df_w1_pca0, df_w1_pca1]\n",
        "\n",
        "for i in range(len(axes_list)):\n",
        "    plot_df_line(ax[axes_list[i][0], axes_list[i][1]],      #Mentioning Axis of figure\n",
        "                    df_list[i], 'total_window', 'recall',   #Dataframe, params to be plotted\n",
        "                    regress=True,\n",
        "                    title=f'(W: {axes_list[i][0]}, pca: {axes_list[i][1]}, L: 45)', \n",
        "                    #yrange=(0, 0.5)\n",
        "                )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
